{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mutual information library from microsoft nlp recipe\n",
    "from Interpreter import calculate_regularization, Interpreter\n",
    "\n",
    "import oks\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import brown\n",
    "#if error exits, maybe you should excute following\n",
    "# python\n",
    "# >> import nltk\n",
    "# >> nltk.download('brown')\n",
    "from model import MyModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word-id lookup table\n",
    "# Get charater-id lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent = []\n",
    "for w in brown.words():\n",
    "    full_sent.append(w)\n",
    "full_sent = ' '.join(full_sent)\n",
    "character_vocab = set(full_sent)\n",
    "char_len = len(character_vocab)\n",
    "\n",
    "word_vocab = []\n",
    "for w in brown.words():\n",
    "    word_vocab.append(w)\n",
    "word_vocab = set(word_vocab)\n",
    "word_len = len(word_vocab)\n",
    "\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "for idx, word in enumerate(word_vocab):\n",
    "    #print('%d, %s' % (idx,word))\n",
    "    word_to_idx[word] = idx\n",
    "    idx_to_word[idx] = word\n",
    "\n",
    "    \n",
    "char_to_idx = {}\n",
    "idx_to_char = {}\n",
    "for idx, char in enumerate(character_vocab):\n",
    "    char_to_idx[char] = idx\n",
    "    idx_to_char[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(gram_len,sent):\n",
    "    '''\n",
    "    input: number of gram, a sentence(str)\n",
    "    return: n-grams\n",
    "    '''\n",
    "    n_gram_pair = []\n",
    "    sent_len = len(sent)\n",
    "    for idx, w in enumerate(sent):\n",
    "        if  idx+gram_len > sent_len:\n",
    "            break\n",
    "        gram = sent[idx:idx+gram_len]\n",
    "        n_gram_pair.append(gram)\n",
    "    return n_gram_pair\n",
    "\n",
    "\n",
    "def get_input_output(n_gram_pair):\n",
    "    '''\n",
    "    input: n-grams\n",
    "    return: input n-1 gram and output 1-gram\n",
    "    '''\n",
    "    input_list = []\n",
    "    output_list = []\n",
    "    for pair in n_gram_pair:\n",
    "        input_str = pair[:-1]\n",
    "        output_str = pair[-1]\n",
    "        input_list.append(' '.join(input_str))\n",
    "        output_list.append(output_str)\n",
    "    return (input_list, output_list)\n",
    "\n",
    "def convert_into_tensor(word):\n",
    "    '''\n",
    "    input: a word(str)\n",
    "    return: character level word ids(tensor)\n",
    "    '''\n",
    "    look = []\n",
    "    for c in word:\n",
    "        look.append(char_to_idx[c])\n",
    "    np_look = np.array(look)  \n",
    "    return torch.from_numpy(np_look)\n",
    "\n",
    "def convert_into_numpy(word): \n",
    "    '''\n",
    "    input: a word(str)\n",
    "    return: character level word ids(numpy)\n",
    "    '''\n",
    "    look = []\n",
    "    for c in word:\n",
    "        look.append(char_to_idx[c])\n",
    "    np_look = np.array(look)  \n",
    "    return np_look#torch.from_numpy(np_look)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate n-1 gram(input) and 1-gram(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_pair = []\n",
    "for sent in brown.sents():\n",
    "    n_gram_pair += n_gram(2,sent)\n",
    "input_, output_ = get_input_output(n_gram_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sample function to get representation from morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phi_simple(x):\n",
    "    '''\n",
    "    input: morphs vector\n",
    "    return: summrised a single representation vector\n",
    "    '''\n",
    "    summarized_lstm,_ = torch.max(x, 0)\n",
    "    return summarized_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel()\n",
    "device = 'cpu'\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2334.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MI = 0\n",
    "\n",
    "for target_word in input_[9:10]:\n",
    "    # Do not update CNN network while update perturbation's noise\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    #net.eval()\n",
    "\n",
    "    # morph_representation: all vectors for representation\n",
    "    # morphs: morphs(str)\n",
    "    morph_representation, morphs = net(target_word)\n",
    "\n",
    "    # calculate the regularization term\n",
    "    # the same as nlp recipe\n",
    "    regularization_simple = calculate_regularization(morph_representation, Phi_simple, device=device)\n",
    "    interpreter_simple = Interpreter(\n",
    "        x=morph_representation,\n",
    "        Phi=Phi_simple,\n",
    "        regularization=regularization_simple,\n",
    "        scale=10 * 0.1,\n",
    "        words=morphs,\n",
    "    )\n",
    "    #a = interpreter_simple.to(device)\n",
    "    interpreter_simple.optimize(iteration=1000, lr=0.01, show_progress=True)\n",
    "\n",
    "    # sigma_numbers: the estimated value of noise variance\n",
    "    sigma_numbers = interpreter_simple.get_sigma()\n",
    "    \n",
    "    # tmp: a single mutual information approximated by moise variance(perturbation theory)\n",
    "    tmp = len(sigma_numbers)*np.log(2*np.pi*math.e)/2.0  + len(sigma_numbers)*np.sum(np.log(sigma_numbers+0.02))# MI = p(sigma)\n",
    "    MI += tmp\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = True\n",
    "    net.train()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other script( less important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A singel loop for mutual information estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_representation, morphs = net(target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#morph_representation.size()\n",
    "#morphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "a = Phi_simple(morph_representation)\n",
    "print(a.size())\n",
    "print(morph_representation.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interpreter()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the regularization term\n",
    "device = 'cpu'\n",
    "regularization_simple = calculate_regularization(morph_representation, Phi_simple, device=device)\n",
    "\n",
    "interpreter_simple = Interpreter(\n",
    "    x=morph_representation,\n",
    "    Phi=Phi_simple,\n",
    "    regularization=regularization_simple,\n",
    "    scale=10 * 0.1,\n",
    "    words=morphs,\n",
    ")\n",
    "interpreter_simple.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter_simple.optimize(iteration=2000, lr=0.01, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sigma we get\n",
    "sigma_numbers = interpreter_simple.get_sigma()\n",
    "sigma_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the information loss of our sigma\n",
    "interpreter_simple.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.Embedding(char_len, 30)\n",
    "w_embeds = nn.Embedding(word_len,100)\n",
    "\n",
    "conv1 = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1)\n",
    "conv2 = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1)\n",
    "conv3 = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1)\n",
    "conv4 = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1)\n",
    "conv_seg = nn.Conv2d(1,1,kernel_size=2,stride=1)\n",
    "sig = nn.Sigmoid()\n",
    "linear = nn.Linear(29,1)\n",
    "lstm = nn.LSTM(30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,inputs, output, char_vocab, word_vocab,max_word_len=40,padding_char='#'):\n",
    "        root = './'\n",
    "        self.inputs = inputs\n",
    "        self.output = output\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.inputs[index], torch.tensor(word_to_idx[self.output[index]])\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.char_embeds = nn.Embedding(char_len, 30)\n",
    "        self.word_embeds = nn.Embedding(word_len,100)\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1,1,kernel_size=2,stride=1),\n",
    "        )\n",
    "        self.linear = nn.Linear(29,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.lstm = nn.LSTM(30, 100)\n",
    "    def forward(self,x):\n",
    "        network_outputs = []\n",
    "        for w in x:\n",
    "            original = convert_into_tensor(w)\n",
    "            char_embedding = self.char_embeds(original)\n",
    "            lstm_out = []\n",
    "            if char_embedding.size()[0] == 1:\n",
    "                _, out2 = self.lstm(char_embedding.unsqueeze(1))\n",
    "                lstm_out.append(out2[0].view(1,-1))    \n",
    "                lstm_out = torch.cat(lstm_out,dim=0)\n",
    "                summarized_lstm,_ = torch.max(lstm_out, 0)\n",
    "                network_outputs.append(summarized_lstm.unsqueeze(0))\n",
    "                continue\n",
    "    \n",
    "            a = char_embedding.unsqueeze(0)\n",
    "            a = a.unsqueeze(0)\n",
    "            seg = self.features(a)\n",
    "            seg = self.linear(seg)\n",
    "            seg = self.sig(seg)\n",
    "        \n",
    "            idx = torch.where(seg >= 0.505)\n",
    "            boundries = idx[2] + 1\n",
    "            outputs = []\n",
    "            outputs.append(torch.tensor(0).unsqueeze(0))\n",
    "\n",
    "            outputs.append(boundries)\n",
    "            outputs.append(torch.tensor(original.size()[0]).unsqueeze(0))\n",
    "            result = torch.cat(outputs, dim=0)\n",
    "            morph = []\n",
    "            for cnt, idx in enumerate(result):\n",
    "                if(cnt == 0):\n",
    "                    continue\n",
    "                morph.append(char_embedding[result[cnt-1]:result[cnt]].unsqueeze(1))\n",
    "\n",
    "    \n",
    "            for m in morph:\n",
    "                _, out2 = self.lstm(m)\n",
    "                lstm_out.append(out2[0].view(1,-1))\n",
    "        \n",
    "            lstm_out = torch.cat(lstm_out,dim=0)\n",
    "            return lstm_out\n",
    "            #summarized_lstm,_ = torch.max(lstm_out, 0)\n",
    "            \n",
    "            #network_outputs.append(summarized_lstm.unsqueeze(0))\n",
    "                \n",
    "        #representation = torch.cat(network_outputs,dim=0)\n",
    "        #out_words_embedding = self.word_embeds(y) \n",
    "        #return representation#out_words_embedding\n",
    "    \n",
    "def get_morph_representation(word,chars):\n",
    "    #original = convert_into_tensor(word)\n",
    "    #print(original.size())\n",
    "    #char_embedding = embeds(original)\n",
    "    lstm_out = []\n",
    "    #if char_embedding.size()[0] == 1:\n",
    "    #    #print('here')\n",
    "    #    #print(char_embedding.size())\n",
    "    #    _, out2 = lstm(char_embedding.unsqueeze(1))\n",
    "    #    lstm_out.append(out2[0].view(1,-1))    \n",
    "    #    lstm_out = torch.cat(lstm_out,dim=0)\n",
    "    #    summarized_lstm,_ = torch.max(lstm_out, 0)\n",
    "    #    return summarized_lstm\n",
    "    \n",
    "    a = word.unsqueeze(0)\n",
    "    #print(a.size())\n",
    "    a = a.unsqueeze(0)\n",
    "    #print(a.size())\n",
    "    a = conv1(a)\n",
    "    a = conv2(a)\n",
    "    a = conv3(a)\n",
    "    a = conv4(a)\n",
    "    seg = conv_seg(a)\n",
    "\n",
    "    seg = linear(seg)\n",
    "    seg = sig(seg)\n",
    "    idx = torch.where(seg >= 0.47)\n",
    "    b = idx[2] + 1\n",
    "    outputs = []\n",
    "    outputs.append(torch.tensor(0).unsqueeze(0))\n",
    "\n",
    "    outputs.append(b)\n",
    "    outputs.append(torch.tensor(word.size()[0]).unsqueeze(0))\n",
    "    result = torch.cat(outputs, dim=0)\n",
    "    morph = []\n",
    "    morph_char = []\n",
    "    for cnt, idx in enumerate(result):\n",
    "        if(cnt == 0):\n",
    "            continue\n",
    "        morph.append(word[result[cnt-1]:result[cnt]].unsqueeze(1))\n",
    "        morph_char.append(chars[result[cnt-1]:result[cnt]])\n",
    "\n",
    "\n",
    "    \n",
    "    for m in morph:\n",
    "        _, out2 = lstm(m)\n",
    "        lstm_out.append(out2[0].view(1,-1))\n",
    "    \n",
    "    #return lstm_out\n",
    "    lstm_out = torch.cat(lstm_out,dim=0)\n",
    "    #summarized_lstm,_ = torch.max(lstm_out, 0)\n",
    "    #summarized_lstm.size()\n",
    "    return lstm_out,morph_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(input_[:2000],output_[:2000],character_vocab, word_vocab)\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,batch_size=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MyModel()\n",
    "#net = net.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0843,  0.4593, -0.0694,  ...,  0.1333, -0.8772,  0.1286],\n",
      "        [-0.0112, -0.0399,  0.7193,  ...,  0.2070,  0.3534, -0.8822],\n",
      "        [ 0.9462,  0.5140, -1.6100,  ..., -0.9133, -0.3494, -0.1511],\n",
      "        ...,\n",
      "        [ 2.2396,  0.5679, -0.2022,  ..., -0.2242, -0.7294,  2.0772],\n",
      "        [-0.7914,  0.6227, -1.2852,  ..., -0.4904,  0.3192,  1.4718],\n",
      "        [ 0.2730, -0.0218, -0.1653,  ...,  0.9036,  1.0216, -2.2563]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2081,  1.9920, -1.3581,  ...,  0.3201,  0.2311, -0.7095],\n",
      "        [ 0.6023,  0.3342,  1.4850,  ...,  0.7748,  1.1667,  0.2439],\n",
      "        [ 0.5801,  0.1347, -0.1595,  ..., -0.8043,  1.0215, -1.1601],\n",
      "        ...,\n",
      "        [-0.5728, -1.4312, -0.2980,  ..., -0.7440, -1.4177,  1.6842],\n",
      "        [ 0.4065,  0.2546,  1.2468,  ...,  1.0366,  0.2595,  0.5468],\n",
      "        [-0.3109,  0.2271, -0.6665,  ...,  0.5310,  1.6688,  1.9888]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.3268,  0.2377, -0.0823],\n",
      "          [ 0.2618,  0.1738, -0.1690],\n",
      "          [ 0.0056,  0.1877, -0.2138]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0363], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1120, -0.1409,  0.0875],\n",
      "          [-0.2575,  0.2945, -0.1589],\n",
      "          [-0.3195,  0.0073,  0.1823]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2284], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1622, -0.0427, -0.1410],\n",
      "          [-0.1743, -0.2411,  0.3244],\n",
      "          [ 0.1395,  0.2226,  0.0103]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2108], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0840,  0.0430,  0.0287],\n",
      "          [-0.3325, -0.0155, -0.2976],\n",
      "          [ 0.1010, -0.0225,  0.0273]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1737], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[0.1646, 0.1988],\n",
      "          [0.1329, 0.0991]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2505], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0491, -0.0926, -0.1401, -0.1428, -0.0204,  0.1176,  0.0914,  0.1728,\n",
      "         -0.1824,  0.1111,  0.0420,  0.0375,  0.0098,  0.0233, -0.0367,  0.0809,\n",
      "         -0.0676, -0.1188,  0.0745, -0.1271,  0.1434,  0.0897,  0.1290, -0.1309,\n",
      "         -0.1536, -0.1083,  0.1763,  0.1189, -0.1333]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1186], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0325, -0.0144,  0.0320,  ...,  0.0940,  0.0527,  0.0079],\n",
      "        [ 0.0063,  0.0093,  0.0841,  ...,  0.0322,  0.0603, -0.0496],\n",
      "        [-0.0898,  0.0504,  0.0489,  ..., -0.0821,  0.0955,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0547, -0.0293,  0.0064,  ..., -0.0853,  0.0194, -0.0812],\n",
      "        [-0.0308, -0.0746,  0.0648,  ..., -0.0897,  0.0821, -0.0711],\n",
      "        [ 0.0359, -0.0037, -0.0926,  ..., -0.0689, -0.0483,  0.0750]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0349, -0.0419, -0.0659,  ...,  0.0156, -0.0883,  0.0222],\n",
      "        [-0.0610, -0.0800,  0.0952,  ...,  0.0281, -0.0649,  0.0332],\n",
      "        [ 0.0753,  0.0388, -0.0537,  ...,  0.0878, -0.0053,  0.0141],\n",
      "        ...,\n",
      "        [ 0.0890, -0.0134, -0.0002,  ..., -0.0244, -0.0974,  0.0681],\n",
      "        [ 0.0066,  0.0964, -0.0358,  ...,  0.0012,  0.0581,  0.0341],\n",
      "        [-0.0574,  0.0511, -0.0965,  ..., -0.0480,  0.0447,  0.0157]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0141, -0.0205,  0.0853,  0.0639, -0.0836, -0.0756,  0.0403,  0.0384,\n",
      "         0.0607,  0.0574, -0.0387,  0.0645, -0.0103,  0.0388, -0.0424,  0.0108,\n",
      "         0.0065, -0.0780, -0.0323,  0.0062, -0.0934, -0.0771, -0.0560,  0.0125,\n",
      "        -0.0806,  0.0540, -0.0676,  0.0316,  0.0553,  0.0520, -0.0620,  0.0958,\n",
      "        -0.0438,  0.0008,  0.0928,  0.0051, -0.0728,  0.0968,  0.0599,  0.0916,\n",
      "        -0.0570, -0.0886, -0.0708,  0.0704,  0.0170,  0.0163, -0.0630,  0.0213,\n",
      "         0.0263, -0.0780, -0.0705,  0.0622, -0.0960, -0.0631,  0.0791,  0.0387,\n",
      "         0.0643, -0.0956, -0.0313, -0.0500, -0.0734, -0.0282, -0.0868,  0.0741,\n",
      "         0.0011,  0.0860, -0.0896, -0.0401,  0.0021,  0.0747,  0.0148, -0.0302,\n",
      "         0.0241,  0.0773,  0.0950, -0.0135, -0.0641,  0.0327, -0.0381,  0.0109,\n",
      "        -0.0803, -0.0112,  0.0388,  0.0652, -0.0726,  0.0986, -0.0799, -0.0879,\n",
      "        -0.0843, -0.0616, -0.0067,  0.0142, -0.0993,  0.0906,  0.0359, -0.0076,\n",
      "         0.0258, -0.0557, -0.0984,  0.0591,  0.0936, -0.0151, -0.0677,  0.0238,\n",
      "         0.0313,  0.0758, -0.0790, -0.0798,  0.0198, -0.0580, -0.0164, -0.0903,\n",
      "         0.0329, -0.0223, -0.0339,  0.0192, -0.0210,  0.0147, -0.0948,  0.0559,\n",
      "        -0.0445,  0.0868, -0.0409, -0.0959,  0.0602,  0.0350,  0.0711,  0.0636,\n",
      "         0.0120, -0.0122, -0.0572,  0.0143, -0.0191,  0.0900,  0.0286,  0.0733,\n",
      "        -0.0905, -0.0632,  0.0291,  0.0549,  0.0153, -0.0039,  0.0566, -0.0246,\n",
      "         0.0253,  0.0465, -0.0605, -0.0060, -0.0943, -0.0709, -0.0806,  0.0496,\n",
      "        -0.0275, -0.0153,  0.0879,  0.0799, -0.0180,  0.0106, -0.0310, -0.0181,\n",
      "        -0.0144,  0.0571,  0.0717,  0.0873,  0.0426,  0.0499,  0.0829, -0.0972,\n",
      "        -0.0758,  0.0328, -0.0130,  0.0286, -0.0393,  0.0615,  0.0755,  0.0394,\n",
      "         0.0268, -0.0180,  0.0229, -0.0383,  0.0430,  0.0384,  0.0098, -0.0532,\n",
      "        -0.0416,  0.0435, -0.0989, -0.0581, -0.0648,  0.0367, -0.0037, -0.0085,\n",
      "         0.0643,  0.0758, -0.0752, -0.0648, -0.0669,  0.0564,  0.0811,  0.0412,\n",
      "        -0.0864,  0.0771,  0.0251,  0.0085, -0.0014, -0.0069,  0.0781,  0.0244,\n",
      "         0.0414, -0.0602, -0.0316,  0.0458,  0.0337,  0.0144,  0.0472,  0.0541,\n",
      "        -0.0403, -0.0234,  0.0379, -0.0279,  0.0808,  0.0905,  0.0972,  0.0386,\n",
      "         0.0921,  0.0671, -0.0952,  0.0533,  0.0720, -0.0060, -0.0779,  0.0946,\n",
      "         0.0163, -0.0740,  0.0689,  0.0319,  0.0700,  0.0279,  0.0208,  0.0788,\n",
      "        -0.0792, -0.0800,  0.0575,  0.0593, -0.0386, -0.0331,  0.0299,  0.0360,\n",
      "        -0.0591, -0.0525,  0.0715, -0.0288,  0.0865, -0.0275, -0.0288,  0.0037,\n",
      "        -0.0807, -0.0885, -0.0129, -0.0181,  0.0830, -0.0521,  0.0734,  0.0603,\n",
      "        -0.0234, -0.0662, -0.0385, -0.0534,  0.0724, -0.0015,  0.0465,  0.0830,\n",
      "        -0.0344, -0.0957,  0.0343, -0.0062,  0.0367, -0.0844,  0.0706, -0.0984,\n",
      "         0.0419, -0.0293, -0.0126,  0.0897, -0.0275, -0.0186, -0.0754,  0.0402,\n",
      "         0.0163,  0.0437, -0.0740,  0.0932,  0.0414, -0.0272, -0.0582,  0.0122,\n",
      "        -0.0087, -0.0391, -0.0804,  0.0968, -0.0927,  0.0598, -0.0573,  0.0705,\n",
      "         0.0156, -0.0012,  0.0950,  0.0423, -0.0044,  0.0696, -0.0787,  0.0656,\n",
      "        -0.0471, -0.0563, -0.0564, -0.0623, -0.0674, -0.0638, -0.0224,  0.0228,\n",
      "        -0.0309,  0.0535,  0.0506, -0.0322,  0.0770, -0.0503,  0.0125,  0.0922,\n",
      "         0.0661,  0.0971,  0.0204,  0.0949,  0.0267,  0.0420, -0.0893, -0.0861,\n",
      "        -0.0975, -0.0121,  0.0280, -0.0488,  0.0280, -0.0486,  0.0210, -0.0524,\n",
      "         0.0077,  0.0025,  0.0390,  0.0055,  0.0190, -0.0579, -0.0878, -0.0675,\n",
      "        -0.0941,  0.0072, -0.0726, -0.0386,  0.0020,  0.0225,  0.0424, -0.0777,\n",
      "         0.0103,  0.0797,  0.0817,  0.0238, -0.0410,  0.0523,  0.0987, -0.0302,\n",
      "        -0.0557,  0.0823,  0.0600,  0.0519,  0.0881,  0.0331,  0.0394,  0.0967,\n",
      "        -0.0209,  0.0576,  0.0477, -0.0584,  0.0897,  0.0349, -0.0080,  0.0136,\n",
      "        -0.0495,  0.0522, -0.0048,  0.0315, -0.0553, -0.0545,  0.0007, -0.0006,\n",
      "         0.0399,  0.0199,  0.0859, -0.0404, -0.0752,  0.0739, -0.0872, -0.0341],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0446, -0.0263,  0.0660,  0.0479,  0.0095,  0.0323, -0.0930, -0.0324,\n",
      "         0.0303, -0.0197, -0.0958, -0.0702,  0.0735,  0.0976,  0.0809,  0.0386,\n",
      "        -0.0364,  0.0050,  0.0760,  0.0850,  0.0100,  0.0026,  0.0221, -0.0027,\n",
      "         0.0164,  0.0004, -0.0268, -0.0713,  0.0953, -0.0084, -0.0271, -0.0979,\n",
      "         0.0472,  0.0853,  0.0202,  0.0502, -0.0220,  0.0486, -0.0522,  0.0105,\n",
      "        -0.0594, -0.0652, -0.0252, -0.0123,  0.0999,  0.0850, -0.0992,  0.0995,\n",
      "        -0.0783, -0.0344,  0.0764,  0.0830,  0.0659, -0.0008, -0.0873, -0.0667,\n",
      "        -0.0371,  0.0827,  0.0796, -0.0366,  0.0810, -0.0789,  0.0398,  0.0910,\n",
      "        -0.0907, -0.0605,  0.0560,  0.0838, -0.0448, -0.0436,  0.0444, -0.0234,\n",
      "        -0.0514, -0.0990, -0.0234, -0.0978,  0.0195, -0.0194,  0.0408, -0.0944,\n",
      "         0.0616,  0.0793, -0.0147,  0.0487,  0.0749,  0.0592, -0.0885, -0.0756,\n",
      "        -0.0546,  0.0523, -0.0223,  0.0323,  0.0306, -0.0605,  0.0972,  0.0443,\n",
      "        -0.0622, -0.0061, -0.0360, -0.0419, -0.0253, -0.0118,  0.0570, -0.0449,\n",
      "         0.0836,  0.0446,  0.0941, -0.0081, -0.0699,  0.0672, -0.0975, -0.0464,\n",
      "        -0.0864, -0.0768, -0.0583,  0.0525,  0.0091,  0.0055, -0.0368,  0.0898,\n",
      "         0.0802, -0.0242,  0.0032, -0.0471, -0.0268,  0.0176,  0.0615, -0.0811,\n",
      "         0.0164,  0.0072, -0.0245,  0.0020, -0.0359, -0.0962,  0.0724,  0.0108,\n",
      "        -0.0115, -0.0951, -0.0729, -0.0741, -0.0297,  0.0027,  0.0959, -0.0031,\n",
      "         0.0304,  0.0203,  0.0892, -0.0174, -0.0284, -0.0899,  0.0367, -0.0047,\n",
      "         0.0128,  0.0278,  0.0670,  0.0737,  0.0019,  0.0268,  0.0380,  0.0807,\n",
      "         0.0603,  0.0718,  0.0508, -0.0770,  0.0075,  0.0436,  0.0499, -0.0392,\n",
      "         0.0214, -0.0211,  0.0893,  0.0508,  0.0808, -0.0220, -0.0699, -0.0011,\n",
      "         0.0243, -0.0185, -0.0850, -0.0223, -0.0907, -0.0793,  0.0615, -0.0875,\n",
      "         0.0025, -0.0886,  0.0989, -0.0251, -0.0640, -0.0039, -0.0516, -0.0682,\n",
      "        -0.0098,  0.0819,  0.0058, -0.0946,  0.0313,  0.0928, -0.0630, -0.0439,\n",
      "         0.0005,  0.0727, -0.0596, -0.0248,  0.0721,  0.0338,  0.0014, -0.0577,\n",
      "         0.0372, -0.0453,  0.0964,  0.0702,  0.0742, -0.0745,  0.0057,  0.0484,\n",
      "        -0.0384, -0.0952,  0.0158,  0.0614,  0.0930, -0.0063, -0.0020, -0.0933,\n",
      "         0.0042,  0.0108, -0.0935, -0.0686, -0.0507, -0.0153,  0.0620, -0.0644,\n",
      "        -0.0521,  0.0282,  0.0377, -0.0008, -0.0877, -0.0721, -0.0408,  0.0378,\n",
      "        -0.0160,  0.0090, -0.0255, -0.0787, -0.0190, -0.0613, -0.0514,  0.0614,\n",
      "        -0.0519, -0.0721,  0.0090,  0.0448, -0.0507,  0.0699, -0.0096,  0.0700,\n",
      "        -0.0606,  0.0348,  0.0459, -0.0920,  0.0001, -0.0165, -0.0982,  0.0928,\n",
      "         0.0108, -0.0235,  0.0689,  0.0215, -0.0259, -0.0391,  0.0125,  0.0038,\n",
      "        -0.0191,  0.0584,  0.0601, -0.0975, -0.0145, -0.0225,  0.0504,  0.0504,\n",
      "        -0.0105, -0.0682, -0.0839, -0.0023,  0.0390,  0.0995,  0.0995, -0.0471,\n",
      "         0.0439, -0.0657,  0.0853, -0.0867,  0.0474,  0.0749,  0.0631,  0.0283,\n",
      "         0.0805,  0.0038,  0.0340,  0.0901, -0.0963,  0.0082, -0.0042, -0.0120,\n",
      "         0.0919, -0.0311,  0.0612,  0.0116, -0.0612,  0.0742,  0.0899,  0.0951,\n",
      "        -0.0403, -0.0463,  0.0841,  0.0831, -0.0711, -0.0152,  0.0504,  0.0655,\n",
      "         0.0430,  0.0369,  0.0844,  0.0842,  0.0752,  0.0517,  0.0676, -0.0458,\n",
      "         0.0239, -0.0265,  0.0424, -0.0905, -0.0677,  0.0732, -0.0517,  0.0940,\n",
      "        -0.0828,  0.0327, -0.0704,  0.0468,  0.0619,  0.0911, -0.0931,  0.0007,\n",
      "        -0.0028,  0.0956, -0.0222, -0.0096, -0.0463,  0.0391,  0.0909, -0.0419,\n",
      "         0.0465,  0.0970, -0.0972, -0.0392, -0.0812,  0.0916,  0.0486,  0.0359,\n",
      "         0.0367,  0.0695,  0.0652, -0.0113,  0.0832,  0.0790,  0.0962, -0.0549,\n",
      "        -0.0023,  0.0922, -0.0386,  0.0025,  0.0400, -0.0495,  0.0562, -0.0012,\n",
      "        -0.0889, -0.0312,  0.0561, -0.0468, -0.0999,  0.0117,  0.0829,  0.0197,\n",
      "         0.0940, -0.0783, -0.0832,  0.0114,  0.0903, -0.0047, -0.0156, -0.0399,\n",
      "         0.0083, -0.0654,  0.0177, -0.0694, -0.0276,  0.0387, -0.0217,  0.0979],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#a =  'hello'\n",
    "#a[0:3]\n",
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 't', 'l', 'a', 'nt', \"a'\", 's']\n",
      "torch.Size([7, 100])\n"
     ]
    }
   ],
   "source": [
    "target_word = input_[10]\n",
    "word_tensor = convert_into_tensor(target_word)\n",
    "char_embedding = nn.Embedding(char_len, 30)\n",
    "#print(word_tensor)\n",
    "char_embeds =  char_embedding(word_tensor)\n",
    "x_simple = char_embeds\n",
    "#print(char_embeds.size())\n",
    "morph_representation, morphs = get_morph_representation(x_simple,target_word)\n",
    "#morphs = []\n",
    "\n",
    "print(morphs)\n",
    "print(morph_representation.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-765faa33320a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the interpreter by optimizing the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minterpreter_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/python/pytorch_projects/keio_ai/nlp-recipes/utils_nlp/interpreter/Interpreter.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, iteration, lr, show_progress)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mminLoss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mminLoss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Train the interpreter by optimizing the loss\n",
    "retain_graph=True\n",
    "interpreter_simple.optimize(iteration=5000, lr=0.5, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-56584af4571c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregularization_simple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create the interpreter instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# we recommend you to set hyper-parameter *scale* to 10 * Std[word_embedding_weight]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch_projects/keio_ai/nlp-recipes/utils_nlp/interpreter/Interpreter.py\u001b[0m in \u001b[0;36mcalculate_regularization\u001b[0;34m(sampled_x, Phi, reduced_axes, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduced_axes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "regularization_simple = calculate_regularization(input_words, net.forward, device=device)\n",
    "words = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "# create the interpreter instance\n",
    "# we recommend you to set hyper-parameter *scale* to 10 * Std[word_embedding_weight]\n",
    "# 10 * 0.1 in this example\n",
    "interpreter_simple = Interpreter(\n",
    "    x=input_words,\n",
    "    Phi=net.forward,\n",
    "    regularization=regularization_simple,\n",
    "    scale=10 * 0.1,\n",
    "    words=words,\n",
    ")\n",
    "interpreter_simple.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
